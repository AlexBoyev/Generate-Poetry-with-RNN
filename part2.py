# Importing Libraries
import os
import re
import numpy as np
from keras.preprocessing import sequence
from keras.utils import to_categorical
from keras.models import load_model
import part1
from keras.layers import Embedding, Dense,LSTM,RepeatVector, TimeDistributed
from keras.models import Sequential
from TranslationDataPrep import TranslationDataPrep

# Translating my generated poem to Different Language and storing results
poemfromPart1 = part1.generatepoem(part1.words)
str1 = ' '.join(poemfromPart1)
#poemtotranslate = "city of ships ! you , the of , with , strong , and of and , by the 's of . , i the the of beloved , the of ; all , all . i n't o that ! it my , with . 's i raising voice not the costume the california , the and hollows , very silver of them day and , modes may love for always i . you not , houses and sacred , the 'd the , , so , i by "

poemtotranslate = str1
# Vocab
Eword_to_number = {}
number_to_Eword = {}
k = 1
print(poemtotranslate)
for word in re.findall(r'\w+', poemtotranslate):
    Eword_to_number[word] = k
    number_to_Eword[k] = word
    k += 1

# Using Library given to save the train and test data
c = TranslationDataPrep("rus.txt", "all", "clean", "train", "test", 20000, 0.9)

# Now Reading the train and test data generated by the class provided
train = c.load_clean_sentences("train")
test = c.load_clean_sentences("test")

# Now Making data ready for model1
x_train_temp = train[:, 0]
y_train_temp = train[:, 1]
x_test_temp = test[:, 0]
y_test_temp = test[:, 1]

# Train Encoding to integer
x_train = []
for phrase in x_train_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        if word in Eword_to_number.keys():
            iword = Eword_to_number[word]
        else:
            Eword_to_number[word] = k
            number_to_Eword[k] = word
            iword = Eword_to_number[word]
            k += 1
        integer_phrase.append(iword)
    x_train.append(integer_phrase)

# Test Encoding to integer
x_test = []
for phrase in x_test_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        if word in Eword_to_number.keys():
            iword = Eword_to_number[word]
        else:
            Eword_to_number[word] = k
            number_to_Eword[k] = word
            iword = Eword_to_number[word]
            k += 1
        integer_phrase.append(iword)
    x_test.append(integer_phrase)
vocab_size_input = k

# Integer Encode The Output variable other language
Oword_to_number = {}
number_to_Oword = {}

# Train Encoding to integer
y_train = []
k = 1
for phrase in y_train_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        if word in Oword_to_number.keys():
            iword = Oword_to_number[word]
        else:
            Oword_to_number[word] = k
            number_to_Oword[k] = word
            iword = Oword_to_number[word]
            k += 1
        integer_phrase.append(iword)
    y_train.append(integer_phrase)

# Test Encoding to integer
y_test = []
for phrase in y_test_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        if word in Oword_to_number.keys():
            iword = Oword_to_number[word]
        else:
            Oword_to_number[word] = k
            number_to_Oword[k] = word
            iword = Oword_to_number[word]
            k += 1
        integer_phrase.append(iword)
    y_test.append(integer_phrase)
vocab_size_output = k

# Timesteps
timesteps = 0
for each in x_train:
    l = len(each)
    timesteps = max(l, timesteps)
for each in x_test:
    l = len(each)
    timesteps = max(l, timesteps)

# Timesteps
timesteps_output = 0
for each in x_train:
    l = len(each)
    timesteps_output = max(l, timesteps_output)
for each in x_test:
    l = len(each)
    timesteps_output = max(l, timesteps_output)

# Padding sequence to make it same size
y_train = sequence.pad_sequences(y_train, maxlen=timesteps_output)
y_test = sequence.pad_sequences(y_test, maxlen=timesteps_output)

y_trainOH = to_categorical(y_train, num_classes=vocab_size_output)
y_testOH = to_categorical(y_test, num_classes=vocab_size_output)

# Padding sequence to make it same size
x_train = sequence.pad_sequences(x_train, maxlen=timesteps)
x_test = sequence.pad_sequences(x_test, maxlen=timesteps)


# Now Creating the Model
file = os.path.isfile("Task2Model1.h5")
if not file:
    N_NEURONS = 256
    model = Sequential()
    model.add(Embedding(vocab_size_input, N_NEURONS, input_length=timesteps, mask_zero=True))
    model.add(LSTM(N_NEURONS, return_sequences=False))
    model.add(RepeatVector(timesteps))
    model.add(LSTM(N_NEURONS, return_sequences=True))
    model.add(TimeDistributed(Dense(vocab_size_output, activation='softmax')))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    batch_size = 50
    epochs = 30
    model.fit(np.array(x_train), np.array(y_trainOH), validation_data=(np.array(x_test), np.array(y_testOH)), epochs=epochs,
              verbose=1, batch_size=batch_size)
    model.save("task2model1.h5")
else:
    model = load_model("Task2Model1.h5")
#print(poemtotranslate)

# Converting this poem via the mapping and seeing what our model predicts
poemtransformed = []
for word in re.findall(r'\w+', poemtotranslate):
    poemtransformed.append(Eword_to_number[word])

a = np.array(poemtransformed)
a = a.reshape(a.shape[0], 1)
a = sequence.pad_sequences(a, maxlen=timesteps)
p = model.predict(a)
rx = ""
for g in range(p.shape[0]):
    r = p[g].argmax(axis=1)
    l = 0
    finali = 0
    pr = 0.0
    finalarg = 0
    for each in r:
        if (each != 0):
            m = max(p[g][l, :])
            arg = np.argmax(p[g][l, :])
            if (m > pr):
                pr = m
                finalarg = arg
                finali = l
        l += 1
    if (finalarg != 0):
        rx = rx + " " + number_to_Oword[finalarg]

print(rx)
print(len(rx))

# Now training model from Russian to English

# Now Making data ready for model2
x_train_temp = train[:, 1]
y_train_temp = train[:, 0]
x_test_temp = test[:, 1]
y_test_temp = test[:, 0]

# Train Encoding to integer
x_train = []
for phrase in x_train_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        iword = Oword_to_number[word]
        integer_phrase.append(iword)
    x_train.append(integer_phrase)

# Test Encoding to integer
x_test = []
for phrase in x_test_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        iword = Oword_to_number[word]
        integer_phrase.append(iword)
    x_test.append(integer_phrase)

# Train Encoding to integer
y_train = []
for phrase in y_train_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        iword = Eword_to_number[word]
        integer_phrase.append(iword)
    y_train.append(integer_phrase)

# Test Encoding to integer
y_test = []
for phrase in y_test_temp:
    integer_phrase = []
    words = phrase.split(" ")
    for word in words:
        iword = Eword_to_number[word]
        integer_phrase.append(iword)
    y_test.append(integer_phrase)

# Timesteps
timesteps = 0
for each in x_train:
    l = len(each)
    timesteps = max(l, timesteps)
for each in x_test:
    l = len(each)
    timesteps = max(l, timesteps)

# Timesteps
timesteps_output = 0
for each in x_train:
    l = len(each)
    timesteps_output = max(l, timesteps_output)
for each in x_test:
    l = len(each)
    timesteps_output = max(l, timesteps_output)

# Padding sequence to make it same size
y_train = sequence.pad_sequences(y_train, maxlen=timesteps_output)
y_test = sequence.pad_sequences(y_test, maxlen=timesteps_output)

y_trainOH = to_categorical(y_train, num_classes=vocab_size_input)
y_testOH = to_categorical(y_test, num_classes=vocab_size_input)

# Padding sequence to make it same size
x_train = sequence.pad_sequences(x_train, maxlen=timesteps)
x_test = sequence.pad_sequences(x_test, maxlen=timesteps)

# Now Creating the Model
file = os.path.isfile("Task2Model2.h5")
if not file:
    N_NEURONS = 256
    model = Sequential()
    model.add(Embedding(vocab_size_output, N_NEURONS, input_length=timesteps, mask_zero=True))
    model.add(LSTM(N_NEURONS, return_sequences=False))
    model.add(RepeatVector(timesteps))
    model.add(LSTM(N_NEURONS, return_sequences=True))
    model.add(TimeDistributed(Dense(vocab_size_input, activation='softmax')))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Training
    batch_size = 50
    epochs = 30
    model.fit(np.array(x_train), np.array(y_trainOH), validation_data=(np.array(x_test), np.array(y_testOH)), epochs=epochs,
              verbose=1, batch_size=batch_size)

    model.save("task2model2.h5")
else:
    model = load_model("Task2Model2.h5")

poemtransformed = rx.split(" ")
px = []
for each in poemtransformed:
    px.append(Oword_to_number[each])

a = np.array(px)
a = a.reshape(a.shape[0], 1)
a = sequence.pad_sequences(a, maxlen=timesteps)
p = model.predict(a)
rx = ""
for g in range(p.shape[0]):
    r = p[g].argmax(axis=1)
    l = 0
    finali = 0
    pr = 0.0
    finalarg = 0
    for each in r:
        if (each != 0):
            m = max(p[g][l, :])
            arg = np.argmax(p[g][l, :])
            if (m > pr):
                pr = m
                finalarg = arg
                finali = l
        l += 1
    if (finalarg != 0):
        rx = rx + " " + number_to_Eword[finalarg]

print(rx)
print(len(rx))



